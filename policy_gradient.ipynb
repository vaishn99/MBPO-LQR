{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "import  numpy.random as random\n",
    "from operator import itemgetter\n",
    "from scipy.optimize import nnls\n",
    "import math\n",
    "import control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining required classes\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    def flush_all(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        return\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done,policy):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done,policy)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def push_batch(self, batch):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            append_len = min(self.capacity - len(self.buffer), len(batch))\n",
    "            self.buffer.extend([None] * append_len)\n",
    "\n",
    "        if self.position + len(batch) < self.capacity:\n",
    "            self.buffer[self.position : self.position + len(batch)] = batch\n",
    "            self.position += len(batch)\n",
    "        else:\n",
    "            self.buffer[self.position : len(self.buffer)] = batch[:len(self.buffer) - self.position]\n",
    "            self.buffer[:len(batch) - len(self.buffer) + self.position] = batch[len(self.buffer) - self.position:]\n",
    "            self.position = len(batch) - len(self.buffer) + self.position\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        batch = random.sample(self.buffer, int(batch_size))\n",
    "        state, action, reward, next_state, done,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done,policy\n",
    "\n",
    "    def sample_all_batch(self, batch_size):\n",
    "        idxes = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = list(itemgetter(*idxes)(self.buffer))\n",
    "        state, action, reward, next_state, done,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done,policy\n",
    "\n",
    "    def return_all(self):\n",
    "        return self.buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class ENV:\n",
    "    def __init__(self,A,B,Q,R,target_state=np.zeros(3)):\n",
    "        \n",
    "        self.A=A\n",
    "        self.B=B\n",
    "        self.Q=Q\n",
    "        self.R=R\n",
    "        self.target_state=target_state\n",
    "        self.current_action=None\n",
    "        self.current_state=None\n",
    "        \n",
    "    def reset(self):\n",
    "        # self.current_state=0.1*np.random.rand(3)\n",
    "        self.current_state=0.1*np.zeros(3)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self,action):\n",
    "        mean = self.A@np.array([self.current_state]).T + self.B@action\n",
    "        next_state=np.random.multivariate_normal(mean.T[0],np.eye(len(mean)))\n",
    "        part_1=np.array([next_state])@self.Q@np.array([next_state]).T\n",
    "        part_2=action.T@self.R@action\n",
    "        \n",
    "        self.current_action=action\n",
    "        if np.linalg.norm(next_state-self.target_state)<0.1:\n",
    "            return part_1+part_2,next_state,True\n",
    "        return part_1+part_2,next_state,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=0.8\n",
    "gamma=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_D_real(K_t,env,D_real,L,start_state=None):\n",
    "    D_real.flush_all()\n",
    "    i=0\n",
    "    Is_done=False\n",
    "    env.reset()\n",
    "    while i<L and Is_done!=True:\n",
    "        u_T=K_t@np.array([env.current_state]).T\n",
    "  \n",
    "        u_T=u_T+np.array([np.random.multivariate_normal(mean=np.zeros(3),cov=math.pow(sigma,2)*np.eye(3))]).T\n",
    "\n",
    "        R_t,S_t,Is_done=env.step(u_T)\n",
    "        New_policy=K_t+math.pow(sigma,2)*np.zeros(3) # Modified\n",
    "        D_real.push(env.current_state,u_T,R_t[0][0],S_t,Is_done,New_policy)\n",
    "        env.current_state=S_t\n",
    "        if Is_done:\n",
    "            break\n",
    "        i+=1  \n",
    "    return\n",
    "def get_episodes(D_fake,L):\n",
    "    i=0\n",
    "    globl=[]\n",
    "    locl=[]\n",
    "    j=0\n",
    "    for i in range(D_fake.position):\n",
    "       locl.append(D_fake.buffer[i])\n",
    "       flag=D_fake.buffer[i][4]\n",
    "       i+=1\n",
    "       j+=1\n",
    "       if flag or j==L:\n",
    "           globl.append(locl)\n",
    "           locl=[]\n",
    "           j=0\n",
    "    return globl\n",
    "def Normal_distrb(z, μ, Σ):\n",
    "    \n",
    "    z = np.atleast_2d(z)\n",
    "    μ = np.atleast_2d(μ)\n",
    "    Σ = np.atleast_2d(Σ)\n",
    "\n",
    "    N = z.size\n",
    "\n",
    "    temp1 = np.linalg.det(Σ) ** (-1/2)\n",
    "    temp2 = np.exp(-.5 * (z - μ).T @ np.linalg.inv(Σ) @ (z - μ))\n",
    "\n",
    "    return (2 * np.pi) ** (-N/2) * temp1 * temp2\n",
    "\n",
    "def Get_importance_term(episode,K):\n",
    "    # SARSA\n",
    "    prod=1.0\n",
    "    for i in range(len(episode)):\n",
    "        S_t=episode[i][0]\n",
    "        mean=K@np.array([S_t]).T\n",
    "        var=np.eye(3)*math.pow(sigma,2)\n",
    "        num=Normal_distrb(episode[i][1],mean,var)\n",
    "        mean=episode[i][5]@np.array([S_t]).T\n",
    "        var=np.eye(3)*math.pow(sigma,2)\n",
    "        den=Normal_distrb(episode[i][1],mean,var)\n",
    "        prod=prod*num/den\n",
    "        i+=1\n",
    "    return prod\n",
    "\n",
    "# Term 3\n",
    "def Get_Reward(episode,t):\n",
    "    r=0.0\n",
    "    for i in range(t,len(episode)):\n",
    "        r+=episode[i][2]*math.pow(gamma,i)\n",
    "        i+=1\n",
    "    return r\n",
    "\n",
    "# Term 2\n",
    "\n",
    "def Get_gradient_for_episode(episode,K):\n",
    "    i=0\n",
    "    grad_sum=np.zeros_like(K)\n",
    "    while i<len(episode):\n",
    "        a_t=episode[i][1]\n",
    "        s_t=episode[i][0]\n",
    "        grad_sum+=(Get_gradient_for_tup(a_t,s_t,K)*Get_Reward(episode,i))\n",
    "        i+=1\n",
    "    return grad_sum\n",
    "\n",
    "def Get_gradient_for_tup(a_t,s_t,K):\n",
    "    un_normlised=a_t@np.array([s_t])-K@np.array([s_t]).T@np.array([s_t])\n",
    "    return un_normlised\n",
    "\n",
    "def get_gradient(list_of_episodes,K):\n",
    "    i=0\n",
    "    grad_sum=np.zeros_like(K)\n",
    "    while i<len(list_of_episodes):\n",
    "        term_1=Get_importance_term(list_of_episodes[i],K) # Tested\n",
    "        term_3=Get_gradient_for_episode(list_of_episodes[i],K) # \n",
    "        grad_sum+=(term_1*term_3)\n",
    "        i+=1\n",
    "    rslt=grad_sum/(len(list_of_episodes)*math.pow(sigma,2))\n",
    "    return rslt\n",
    "    \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters of the env\n",
    "# np.random.seed(0)\n",
    "A=0.1*np.random.rand(3,3)\n",
    "\n",
    "# np.random.seed(4)\n",
    "B=0.1*np.random.rand(3,3)\n",
    "\n",
    "# np.random.seed(1)\n",
    "Q=np.diag([1,0.1,0.01])\n",
    "\n",
    "# np.random.seed(3)\n",
    "R=np.diag([1,0.1,0.01])\n",
    "env=ENV(A,B,Q,R)\n",
    "env.reset() # Reset\n",
    "\n",
    "K_star,_,_=control.lqr(A,B,Q,R)\n",
    "\n",
    "A_hat=0.1*np.random.rand(3,3)   # Initial theta\n",
    "B_hat=0.1*np.random.rand(3,3)   # Initial theta\n",
    "Q_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "R_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "\n",
    "# init policy\n",
    "\n",
    "K=np.random.rand(3,3)   # Initial phi\n",
    "K_t=K\n",
    "\n",
    "D_real = ReplayMemory(10000)   # Real dataA\n",
    "D_fake = ReplayMemory(10000)  # Fake data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.94237918e-02  2.33502230e-01 -1.69306423e-03]\n",
      " [-8.45803493e-01  2.76048742e+00 -4.52276415e-02]\n",
      " [ 1.08134582e+01  9.60079300e-01  1.18210524e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Print K_Star\n",
    "print(K_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diplay_the_param(A,B,Q,R,step_size,L):\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"A matrix is \\n\")\n",
    "    print(A)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"B matrix is \\n\")\n",
    "    print(B)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"Q matrix is \\n\")\n",
    "    print(Q)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"R matrix is \\n\")\n",
    "    print(R)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\" step_size is  \\n\")\n",
    "    print(step_size)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"L value is \\n\")\n",
    "    print(L)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient- Grid search over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is  0\n",
      "Counter is  1\n",
      "Counter is  2\n",
      "Counter is  3\n",
      "Counter is  4\n",
      "Counter is  5\n",
      "Counter is  6\n",
      "Counter is  7\n",
      "Counter is  8\n",
      "Counter is  9\n"
     ]
    }
   ],
   "source": [
    "K=np.diag(np.random.rand(3))   # Initial phi\n",
    "# K_t=K_star\n",
    "K_t=K\n",
    "L=60\n",
    "L_list=[20,40,60,80]\n",
    "step_size_list=[math.pow(0.1,2),math.pow(0.1,3),math.pow(0.1,4)]\n",
    "step_size=math.pow(0.1,2)\n",
    "T=10000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init policy\n",
    "count=0\n",
    "\n",
    "while count<10:\n",
    "    print(\"Counter is \",count)\n",
    "    # True parameters of the env\n",
    "    # np.random.seed(0)\n",
    "    A=0.1*np.random.rand(3,3)\n",
    "\n",
    "    # np.random.seed(4)\n",
    "    B=0.1*np.random.rand(3,3)\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    Q=np.diag([1,0.1,0.01])\n",
    "\n",
    "    # np.random.seed(3)\n",
    "    R=np.diag([1,0.1,0.01])\n",
    "    env=ENV(A,B,Q,R)\n",
    "    env.reset() # Reset\n",
    "\n",
    "    K_star,_,_=control.lqr(A,B,Q,R)\n",
    "    t=0.1*np.random.rand(3,3)   # Initial theta\n",
    "    B_hat=0.1*np.random.rand(3,3)   # Initial theta\n",
    "    Q_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "    R_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "    \n",
    "    for l in L_list:\n",
    "        for step in step_size_list:\n",
    "            L=l\n",
    "            step_size=step\n",
    "        \n",
    "            K=np.random.rand(3,3)   # Initial phi\n",
    "            K_t=K\n",
    "            D_real.flush_all()\n",
    "            D_fake.flush_all()\n",
    "            env.reset()\n",
    "            error_list=[]\n",
    "\n",
    "            for t in range(T):\n",
    "                D_real.flush_all()\n",
    "                update_D_real(K_t,env,D_real,L,None)\n",
    "                list_of_episodes=get_episodes(D_real,L)    \n",
    "                K_t=K_t-(step_size*get_gradient(list_of_episodes,K_t))\n",
    "                if t%10==0:\n",
    "                    m=np.linalg.norm(K_t-K_star)\n",
    "                    error_list.append(m)\n",
    "            if min(np.array(error_list))<1:\n",
    "                print(\"Found a good set\") # a function to display \n",
    "                Diplay_the_param(A,B,Q,R,step_size,L)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple policy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.229409185231964\n",
      "10.785115308735614\n",
      "10.803368766158815\n",
      "10.85452881094977\n",
      "10.842238215763823\n",
      "10.73952091442029\n",
      "10.81384903186435\n",
      "10.72828579040138\n",
      "10.827482036877061\n",
      "10.683984648583179\n",
      "10.816427400826598\n",
      "10.869685463926597\n",
      "10.995933984373107\n",
      "11.220048131827662\n",
      "11.098584356306215\n",
      "11.07124866981851\n",
      "11.069939534774567\n",
      "10.98325532904048\n",
      "10.883747192461955\n",
      "10.830533694679081\n",
      "10.883201767552086\n",
      "11.260779225855694\n",
      "11.203337721808962\n",
      "11.353916586230959\n",
      "11.503543420681178\n",
      "11.51571195898706\n",
      "11.557851717522242\n",
      "11.627425618399236\n",
      "11.597522390838712\n",
      "11.631922508868646\n",
      "11.60446417362614\n",
      "11.65873297786414\n",
      "11.472903769523022\n",
      "11.541904984648818\n",
      "11.464379136688764\n",
      "11.50664429724758\n",
      "11.562908588768607\n",
      "11.548562358387576\n",
      "11.68872359871662\n",
      "11.729774315290337\n",
      "11.579080487444875\n",
      "11.475109025079036\n",
      "11.434014490925016\n",
      "11.57393817236187\n",
      "11.457521288164154\n",
      "11.436555820070184\n",
      "11.387931790523055\n",
      "11.328278293172898\n",
      "11.489948583448781\n",
      "11.485570457696644\n",
      "11.57120304678667\n",
      "11.730772046577133\n",
      "11.819022915146348\n",
      "11.669432614388231\n",
      "11.637533949599305\n",
      "11.698723053987557\n",
      "11.750762683669988\n",
      "11.845634179436782\n",
      "11.752515894761041\n",
      "11.569852416972978\n",
      "11.534363142545216\n",
      "11.423211015248425\n",
      "11.60901417606227\n",
      "11.634169506366563\n",
      "11.618906757740318\n",
      "11.607925136656077\n",
      "11.61274766175021\n",
      "11.805499573208902\n",
      "11.847609994938935\n",
      "11.870081404540798\n",
      "11.951323909816859\n",
      "11.967627379636827\n",
      "12.004962673923304\n",
      "12.116263537037106\n",
      "12.140140278520017\n",
      "12.133366306527446\n",
      "12.272792661877672\n",
      "12.362103646379936\n",
      "12.367003102280455\n",
      "12.451151484533959\n",
      "12.50879183159461\n",
      "12.392106817831714\n",
      "12.656938456024772\n",
      "12.444257542779281\n",
      "12.332153746064629\n",
      "12.13282232263137\n",
      "11.954246745381864\n",
      "11.893511912846954\n",
      "11.838347163023064\n",
      "11.714996157507677\n",
      "11.685221330389322\n",
      "11.596771585983273\n",
      "11.56967794476667\n",
      "11.727471417108337\n",
      "11.99518372183345\n",
      "11.86827560672628\n",
      "11.878875212984811\n",
      "12.107933401494334\n",
      "12.162296435209191\n",
      "12.17294274082337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L=10\n",
    "step_size=math.pow(0.1,2)\n",
    "T=100000\n",
    "\n",
    "K=np.random.rand(3,3)   # Initial phi\n",
    "K_t=K\n",
    "D_real.flush_all()\n",
    "D_fake.flush_all()\n",
    "env.reset()\n",
    "error_list=[]\n",
    "\n",
    "for t in range(T):\n",
    "    D_real.flush_all()\n",
    "    update_D_real(K_t,env,D_real,L,None)\n",
    "    list_of_episodes=get_episodes(D_real,L)\n",
    "    # for i in range(10):    \n",
    "    #     K_t=K_t-(step_size*get_gradient(list_of_episodes,K_t))\n",
    "    K_t=K_t-(step_size*get_gradient(list_of_episodes,K_t))\n",
    "    if t%1000==0:\n",
    "        m=np.linalg.norm(K_t-K_star)\n",
    "        error_list.append(m)\n",
    "        print(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
