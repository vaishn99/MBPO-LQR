{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "import  numpy.random as random\n",
    "from operator import itemgetter\n",
    "from scipy.optimize import nnls\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some global parameters\n",
    "\n",
    "horiz_len=10\n",
    "num_of_rollouts=10\n",
    "lamb=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining required classes\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    def flush_all(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        return\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done,policy):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done,policy)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def push_batch(self, batch):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            append_len = min(self.capacity - len(self.buffer), len(batch))\n",
    "            self.buffer.extend([None] * append_len)\n",
    "\n",
    "        if self.position + len(batch) < self.capacity:\n",
    "            self.buffer[self.position : self.position + len(batch)] = batch\n",
    "            self.position += len(batch)\n",
    "        else:\n",
    "            self.buffer[self.position : len(self.buffer)] = batch[:len(self.buffer) - self.position]\n",
    "            self.buffer[:len(batch) - len(self.buffer) + self.position] = batch[len(self.buffer) - self.position:]\n",
    "            self.position = len(batch) - len(self.buffer) + self.position\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        batch = random.sample(self.buffer, int(batch_size))\n",
    "        state, action, reward, next_state, done,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done,policy\n",
    "\n",
    "    def sample_all_batch(self, batch_size):\n",
    "        idxes = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = list(itemgetter(*idxes)(self.buffer))\n",
    "        state, action, reward, next_state, done,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done,policy\n",
    "\n",
    "    def return_all(self):\n",
    "        return self.buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class ENV:\n",
    "    def __init__(self,A,B,Q,R,target_state=np.zeros(3)):\n",
    "        \n",
    "        self.A=A\n",
    "        self.B=B\n",
    "        self.Q=Q\n",
    "        self.R=R\n",
    "        self.target_state=target_state\n",
    "        self.current_action=None\n",
    "        self.current_state=None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state=np.array([0.4,-0.6,0.2])\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self,action):\n",
    "        mean = self.A@np.array([self.current_state]).T + self.B@action\n",
    "        next_state=np.random.multivariate_normal(mean.T[0],np.eye(len(mean)))\n",
    "        part_1=np.array([next_state])@self.Q@np.array([next_state]).T\n",
    "        part_2=action.T@self.R@action\n",
    "        \n",
    "        self.current_action=action\n",
    "        if np.linalg.norm(next_state-self.target_state)<0.1:\n",
    "            return part_1+part_2,next_state,True\n",
    "        \n",
    "        return part_1+part_2,next_state,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_Estimate(D_real):\n",
    "    \n",
    "    # Need to figure out the problem.\n",
    "    \n",
    "    if len(D_real)==0:\n",
    "        return -1\n",
    "    dim_state=D_real.buffer[0][0].shape[0]\n",
    "    dim_action=D_real.buffer[0][1].shape[0]\n",
    "    \n",
    "    A_T=np.zeros(shape=(dim_state+dim_action,len(D_real)))\n",
    "    b_T=np.zeros(shape=(dim_state,len(D_real)))\n",
    "    for i in range(len(D_real)):\n",
    "        b_T[:,i]=D_real.buffer[i][3]\n",
    "        A_T[:,i]=np.concatenate((D_real.buffer[i][0],D_real.buffer[i][1].T[0]))\n",
    "    A=A_T.T\n",
    "    b=b_T.T\n",
    "    total_hat=lstsq(A,b)[0] # need to split\n",
    "    A_hat=total_hat[:dim_state]\n",
    "    B_hat=total_hat[dim_state:]\n",
    "    \n",
    "    A=np.zeros(shape=(len(D_real),dim_state+dim_action))\n",
    "    b=np.zeros(shape=(len(D_real),1))\n",
    "    for i in range(len(D_real.buffer)):\n",
    "        A[i,:]=np.concatenate((D_real.buffer[i][0],D_real.buffer[i][1].T[0]))\n",
    "        b[i,:]=D_real.buffer[i][2]\n",
    "    A=np.square(A)\n",
    "    total_hat=nnls(A,b)[0]\n",
    "    total_hat=total_hat.T[0]\n",
    "    Q_hat=total_hat[:dim_state]\n",
    "    R_hat=total_hat[dim_state:]\n",
    "    Q_hat=np.diag(Q_hat)\n",
    "    R_hat=np.diag(R_hat)\n",
    "    model= [A_hat,B_hat,Q_hat,R_hat]  \n",
    "    return model\n",
    "\n",
    "def Sample_state(D_real):\n",
    "    ind=np.random.randint(low=0,high=D_real.position) \n",
    "    return D_real.buffer[ind][0]\n",
    "\n",
    "\n",
    "def update_D_fake(S_t,model,K_t,D_fake):\n",
    "    \n",
    "    A_hat,B_hat,Q_hat,R_hat=model[0],model[1],model[2],model[3]\n",
    "    \n",
    "    i=0\n",
    "    holder=[]\n",
    "    Is_done=False\n",
    "    target_state=np.zeros(3)\n",
    "    \n",
    "    while i<horiz_len and Is_done!=True:\n",
    "        prev=S_t\n",
    "        u_T=K_t@np.array([S_t]).T+\n",
    "        S_t=A_hat@np.array([S_t]).T+B_hat@u_T\n",
    "        \n",
    "        next_state=np.random.multivariate_normal(S_t.T[0],np.eye(len(S_t)))\n",
    "        \n",
    "        part_1=np.array([next_state])@Q_hat@np.array([next_state]).T\n",
    "        part_2=u_T.T@R_hat@u_T\n",
    "        S_t=next_state\n",
    "        \n",
    "        if np.linalg.norm(S_t-target_state)<0.01:\n",
    "            Is_done=True\n",
    "        \n",
    "        D_fake.push(prev,u_T,part_1[0][0]+part_2[0][0],next_state,Is_done,K_t)\n",
    "        i+=1\n",
    "    \n",
    "    return \n",
    "\n",
    "def update_D_real(K_t,env,D_real):\n",
    "    \n",
    "    i=0\n",
    "    Is_done=False\n",
    "    while i<horiz_len and Is_done!=True:\n",
    "        u_T=K_t@np.array([env.current_state]).T\n",
    "        R_t,S_t,Is_done=env.step(u_T)\n",
    "        D_real.push(env.current_state,u_T,R_t[0][0],S_t,Is_done,K_t)\n",
    "        env.current_state=S_t\n",
    "        if Is_done:\n",
    "            break\n",
    "        i+=1  \n",
    "    return \n",
    "\n",
    "\n",
    "def mod_New_Estimate(D_real):\n",
    "    \n",
    "    if len(D_real)==0:\n",
    "        return -1\n",
    "    dim_state=D_real.buffer[0][0].shape[0]\n",
    "    dim_action=D_real.buffer[0][1].shape[0]\n",
    "    A_T=np.zeros(shape=(dim_state+dim_action,len(D_real)))\n",
    "    b_T=np.zeros(shape=(dim_state,len(D_real)))\n",
    "    for i in range(len(D_real)):\n",
    "        b_T[:,i]=D_real.buffer[i][3]\n",
    "        A_T[:,i]=np.concatenate((D_real.buffer[i][0],D_real.buffer[i][1].T[0]))\n",
    "    A=A_T.T\n",
    "    b=b_T.T\n",
    "    \n",
    "    # Need to add regulariser term.\n",
    "    \n",
    "    lamb = 1\n",
    "    n_variables = A.shape[1]\n",
    "    \n",
    "    opt_parameters=np.zeros(shape=(dim_state,dim_state+dim_action))\n",
    "    \n",
    "    for i in range(dim_state):\n",
    "        A2 = np.concatenate([A, math.sqrt(lamb)*np.eye(n_variables)])\n",
    "        B2 = np.concatenate([b[:,i], np.zeros(n_variables)])\n",
    "        opt_parameters[i]=lstsq(A2,B2)[0]\n",
    "    A_hat = opt_parameters[:,:dim_state]\n",
    "    B_hat = opt_parameters[:,dim_state:]\n",
    "    \n",
    "    A=np.zeros(shape=(len(D_real),dim_state+dim_action))\n",
    "    b=np.zeros(shape=(len(D_real),1))\n",
    "    for i in range(len(D_real.buffer)):\n",
    "        A[i,:]=np.concatenate((D_real.buffer[i][0],D_real.buffer[i][1].T[0]))\n",
    "        b[i,:]=D_real.buffer[i][2]\n",
    "    A=np.square(A)\n",
    "    total_hat=nnls(A,b.T[0])[0]\n",
    "    Q_hat=np.diag(total_hat[:3])\n",
    "    R_hat=np.diag(total_hat[3:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return [A_hat,B_hat,Q_hat,R_hat]\n",
    "\n",
    "\n",
    "# importance sampling based policy gradient\n",
    "\n",
    "def Normal_distrb(z, μ, Σ):\n",
    "\n",
    "    z = np.atleast_2d(z)\n",
    "    μ = np.atleast_2d(μ)\n",
    "    Σ = np.atleast_2d(Σ)\n",
    "\n",
    "    N = z.size\n",
    "\n",
    "    temp1 = np.linalg.det(Σ) ** (-1/2)\n",
    "    temp2 = np.exp(-.5 * (z - μ).T @ np.linalg.inv(Σ) @ (z - μ))\n",
    "\n",
    "    return (2 * np.pi) ** (-N/2) * temp1 * temp2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad computation-Basic form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/Users/vaishnav/Desktop/Project/MBPO-LQR-main/Screenshot 2022-12-20 at 1.00.21 PM.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second term:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/Users/vaishnav/Desktop/Project/MBPO-LQR-main/grad_term.jpeg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episodes(D_fake):\n",
    "    i=0\n",
    "    globl=[]\n",
    "    locl=[]\n",
    "    j=0\n",
    "    for i in range(D_fake.position):\n",
    "       locl.append(D_fake.buffer[i])\n",
    "       flag=D_fake.buffer[i][4]\n",
    "       i+=1\n",
    "       j+=1\n",
    "       if flag or j==horiz_len:\n",
    "           globl.append(locl)\n",
    "           locl=[]\n",
    "           j=0\n",
    "    return globl\n",
    "\n",
    "# Term 1 \n",
    "def Get_importance_term(episode,K):\n",
    "    prod=1.0\n",
    "    for i in range(len(episode)):\n",
    "        S_t=episode[i][0]\n",
    "        mean=K@np.array([S_t]).T\n",
    "        var=np.eye(3)*lamb\n",
    "        num=Normal_distrb(episode[i][1],mean,var)\n",
    "        mean=episode[i][5]@np.array([S_t]).T\n",
    "        var=np.eye(3)*lamb\n",
    "        den=Normal_distrb(episode[i][1],mean,var)\n",
    "        prod=prod*num/den\n",
    "        i+=1\n",
    "    return prod\n",
    "\n",
    "# Term 3\n",
    "def Get_Reward(episode):\n",
    "    r=0.0\n",
    "    for i in range(len(episode)):\n",
    "        r+=episode[i][2]\n",
    "        i+=1\n",
    "    return r\n",
    "\n",
    "# Term 2\n",
    "def Get_gradient_for_episode(episode,K):\n",
    "    i=0\n",
    "    grad_sum=np.zeros_like(K)\n",
    "    while i<len(episode):\n",
    "        a_t=episode[i][1]\n",
    "        s_t=episode[i][0]\n",
    "        grad_sum+=Get_gradient_for_tup(a_t,s_t,K)\n",
    "        i+=1\n",
    "    return grad_sum\n",
    "\n",
    "def Get_gradient_for_tup(a_t,s_t,K):\n",
    "    un_normlised=a_t@np.array([s_t])-K@np.array([s_t]).T@np.array([s_t])\n",
    "    return un_normlised*(1/lamb)\n",
    "\n",
    "def get_gradient(list_of_episodes,K):\n",
    "    i=0\n",
    "    grad_sum=0\n",
    "    while i<len(list_of_episodes):\n",
    "        term_1=Get_importance_term(list_of_episodes[i],K)\n",
    "        term_2=Get_Reward(list_of_episodes[i])\n",
    "        term_3=Get_gradient_for_episode(list_of_episodes[i],K)\n",
    "        grad_sum+=term_1*term_2*term_3\n",
    "        i+=1\n",
    "    return grad_sum/len(list_of_episodes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   Init STEP #######\n",
    "\n",
    "\n",
    "# A ->>3*3\n",
    "# B ->>3*3\n",
    "# C->eye(3)\n",
    "# K ->>3*3\n",
    "\n",
    "################################################################\n",
    "\n",
    "N=2\n",
    "E=2\n",
    "M=2\n",
    "G=2\n",
    "horiz_len=10\n",
    "num_of_rollouts=10\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "# True parameters of the env\n",
    "np.random.seed(0)\n",
    "A=0.1*np.diag(np.random.rand(3))\n",
    "\n",
    "np.random.seed(4)\n",
    "B=0.1*np.diag(np.random.rand(3))\n",
    "\n",
    "np.random.seed(1)\n",
    "Q=np.diag([1,0.1,0.01])\n",
    "\n",
    "np.random.seed(3)\n",
    "R=np.diag([1,0.1,0.01])\n",
    "env=ENV(A,B,Q,R)\n",
    "env.reset() # Reset\n",
    "\n",
    "K_star=np.array([[1.71734423, 0.        , 0.        ],\n",
    "       [0.        , 2.95253427, 0.        ],\n",
    "       [0.        , 0.        , 1.79613291]])\n",
    "\n",
    "A_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "B_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "Q_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "R_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "\n",
    "# init policy\n",
    "\n",
    "K=np.random.rand(3,3)   # Initial phi\n",
    "K_t=K\n",
    "\n",
    "D_real = ReplayMemory(10000)   # Real data\n",
    "D_fake = ReplayMemory(10000)  # Fake data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_model(A=None,B=None,Q=None,R=None):\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"A matrix is \\n\")\n",
    "    print(A)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"B matrix is \\n\")\n",
    "    print(B)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"Q matrix is \\n\")\n",
    "    print(Q)\n",
    "    \n",
    "    print(\"\\n****************************************************************\\n\")\n",
    "    print(\"R matrix is \\n\")\n",
    "    print(R)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real.buffer[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_model(A,B,Q,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation check -Test\n",
    "\n",
    "\n",
    "env.reset()\n",
    "D_real.flush_all()\n",
    "for i in range(1000):\n",
    "    update_D_real(K_t,env,D_real)   # Update D_real\n",
    "    if i%200==0:\n",
    "        A_hat,B_hat,Q_hat,R_hat=mod_New_Estimate(D_real)\n",
    "        print(\"\\n\")\n",
    "        print(np.linalg.norm(A_hat-A))\n",
    "        print(np.linalg.norm(B_hat-B))\n",
    "        print(np.linalg.norm(Q_hat-Q))\n",
    "        print(np.linalg.norm(R_hat-R))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_Fun(model,A,B,Q,R):\n",
    "    print(np.linalg.norm(model[0]-A))\n",
    "    print(\"\\n\")\n",
    "    print(np.linalg.norm(model[1]-B))\n",
    "    print(\"\\n\")\n",
    "    print(np.linalg.norm(model[2]-Q))\n",
    "    print(\"\\n\")\n",
    "    print(np.linalg.norm(model[3]-R))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue occured: diverging Entries in Matrix,state vector\n",
    "# Solution : Find best set of param for the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   Init STEP #######\n",
    "\n",
    "\n",
    "# A ->>3*3\n",
    "# B ->>3*3\n",
    "# C->eye(3)\n",
    "# K ->>3*3\n",
    "\n",
    "################################################################\n",
    "\n",
    "N=20\n",
    "E=20\n",
    "M=5\n",
    "G=20\n",
    "horiz_len=10\n",
    "num_of_rollouts=10\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "# True parameters of the env\n",
    "np.random.seed(0)\n",
    "A=0.1*np.diag(np.random.rand(3))\n",
    "\n",
    "np.random.seed(4)\n",
    "B=0.1*np.diag(np.random.rand(3))\n",
    "\n",
    "np.random.seed(1)\n",
    "Q=np.diag(np.random.rand(3))\n",
    "\n",
    "np.random.seed(3)\n",
    "R=np.diag([0.5,0.01,0])\n",
    "env=ENV(A,B,Q,R)\n",
    "env.reset() # Reset\n",
    "\n",
    "\n",
    "A_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "B_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "Q_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "R_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "\n",
    "# init policy\n",
    "\n",
    "K=np.random.rand(3,3)   # Initial phi\n",
    "K_t=K\n",
    "\n",
    "D_real = ReplayMemory(10000)   # Real data\n",
    "D_fake = ReplayMemory(10000)  # Fake data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MBPO algorithm\n",
    "\n",
    "env.reset()\n",
    "D_fake.flush_all()\n",
    "D_real.flush_all()\n",
    "K=1*np.random.rand(3,3)   # Initial phi\n",
    "K_t=K\n",
    "A_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "B_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "Q_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "R_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "\n",
    "for n in range(N):\n",
    "    print(\"Value for n\\t\",n)\n",
    "    print(\"\\n\")\n",
    "    print(np.linalg.norm(K_t-K_star))\n",
    "    model=mod_New_Estimate(D_real) # Regression\n",
    "    if model==-1:\n",
    "        model=[A_hat,B_hat,Q_hat,R_hat]  # start \n",
    "    for e in range(E):\n",
    "        # Update D_real\n",
    "        update_D_real(K_t,env,D_real)\n",
    "        for m in range(M):\n",
    "            S_t=Sample_state(D_real)    # Random sampling\n",
    "            update_D_fake(S_t,model,K_t,D_fake) # update_D_fake\n",
    "            # Working till here\n",
    "        for g in range(G):\n",
    "            list_of_episodes=get_episodes(D_fake)   # Change representation\n",
    "            term=get_gradient(list_of_episodes,K_t)\n",
    "            if n==0:\n",
    "                K_t=K_t-math.pow(10,-2)*(term+np.random.multivariate_normal(np.zeros(3),0.01*np.eye(3)))   # unKnown parameter (From trajectories -off policy settings)\n",
    "            else:  \n",
    "                K_t=K_t-math.pow(10,-2)*(term+0.0) # unKnown parameter (From trajectories -off policy settings)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test-1\n",
    "\n",
    "# Objective : Will perform policy gradient with true Data. and see where it is heading towrads\n",
    "\n",
    "env.reset()\n",
    "D_fake.flush_all()\n",
    "D_real.flush_all()\n",
    "K=1*np.random.rand(3,3)   # Initial phi\n",
    "K_t=K\n",
    "A_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "B_hat=0.1*np.diag(np.random.rand(3))   # Initial theta\n",
    "Q_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "R_hat=0.1*np.diag(np.ones(3))   # Initial theta\n",
    "\n",
    "E=80\n",
    "G=80\n",
    "\n",
    "for e in tqdm(range(E)):\n",
    "    # Update D_real\n",
    "    print(len(D_real.buffer))\n",
    "    update_D_real(K_t,env,D_real)\n",
    "    \n",
    "    for g in range(G):\n",
    "        list_of_episodes=get_episodes(D_real)   # Change representation\n",
    "        term=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3017499708805589"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to where we have started with\n",
    "np.linalg.norm(K_t-K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5965319567478944"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared \n",
    "np.linalg.norm(K_star-K_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
